已实现和待实现的 AI 介绍。相关代码位于 `src/ai`.

## 朴素 MCTS 

维护一个搜索树，重复四个阶段：

1. 这个过程的第一步叫**选择（Selection）**。从根节点往下走，每次都选一个“最值得看的子节点”（具体规则稍后说），直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。什么叫做“存在未扩展的子节点”，其实就是指这个局面存在未走过的后续着法。
2. 第二步叫**扩展（Expansion）**，我们给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”，就是还没有试过的一个着法。
3. 第三步是**模拟（Simluation）**。从上面这个没有试过的着法开始，用快速走子策略（Rollout policy）走到底，得到一个胜负结果。按照普遍的观点，快速走子策略适合选择一个棋力很弱但走子很快的策略。因为如果这个策略走得慢（比如用 AlphaGo 的策略网络走棋），虽然棋力会更强，结果会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。
4. 第四步是**回溯（Backpropagation）**。把模拟的结果加到它的所有父节点上。例如第三步模拟的结果是 0/1（代表黑棋失败），那么就把这个节点的所有父节点加上 0/1。

选择公式（ucb1）：

> $$
> score = Q + C\cdot \sqrt{\ln(N)/n}
> $$
>
> 其中 $Q$ 表示估测的胜率，$C$ 是参数，据说取 $\sqrt 2$ 最优。$N$ 是父节点的访问次数，$n$ 是子节点的访问次数。

最终选择访问量最大的节点。

### 局面估值和快速走子

局面估值和快速走子是两个系统，用于对朴素 MCTS 做改进。Alphago 及其之后的大部分 AI 中这两个系统用到深度神经网络实现。

我们先通过一些启发式规则（专家系统）来实现这两个系统，不依赖神经网络，简单地提升一下 MCTS AI 的水平。

每种棋需要不同的启发式规则，不可一概而论，所以我们下面分别讨论

#### 合群

最终比较地盘大小的游戏。有明显的局部性，即一个格子只能直接对自己的局部产生影响。

用和函数进行局面估值。即对每个格子，评估其最终属于先手方的概率，并赋予得分 $\in [-1, 1]$，最终将所有得分相加得到局面估值。

对合群来说，对一个格子的影响最重要的指标是双方的染点数量。所谓染点，就是下了之后能立刻染色的点。其它有影响的因素是双染（连下两手能染）的数量和当前颜色。

对单格的评估，简单的一个实现是，先计算双方对这格的染点数量，如果不相等，认为染点更多的一方占据该格；否则判断该格附近是稠密还是稀疏，如果稀疏则根据5*5内棋子数量计算，否则根据该格当前颜色计算。

对局面的估值就是对所有单格评估的总和。快速走子可以通过比较局面估值来实现。需要注意实现细节以提升性能。

#### 占地

也是一个有局部性的比较地盘大小的游戏。

有一个简单且有效的指标：对于每个格子，直接比较该格及所有相邻格中双方的棋子数量，判定给更多的一方。

快速走子通过比较局面估值的变化来实现。

#### 形象

多兵种杀王棋的风格明显。

第一个指标是棋子估值，即给每种棋子赋予分值，这里直接令王=100，车马象=5，兵=1.

第二个指标是位置估值。这里考虑相对位置形成的模式：如果己方棋子威胁了敌方棋子/空格，就赋予这个己方棋子额外得分。这样做是因为，如果一个己方棋子只威胁了己方棋子，那它实际上是没用的，因为形象棋里没有保护的概念。具体来说，认为威胁王+20，威胁车马象+3，威胁兵+1，威胁空格+0.5.

然后把双方所有估值加起来比较。

快速走子：大体思路仍然是考虑局面估值的变化。王的部分需要比较多的特殊处理。



### 局面估值和快速走子应用到 MCTS

